{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Dropout, Merge, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metrics import evaluate_model\n",
    "from embeddings import average_weights_embeddings, save_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "MAXLEN_SEQ = 550\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "FILTERS = (8, 16, 32, 64)\n",
    "KERNEL_SIZE = (4, 8, 10, 12)\n",
    "BRANCHES = len(FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('training_set.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['target'] = df[['domain1_score', 'domain2_score']].apply(lambda x: np.nanmean(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['essay', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['essay'])\n",
    "sequences = tokenizer.texts_to_sequences(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 41432 уникальных токенов.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Найдено %s уникальных токенов.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=MAXLEN_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['target'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n"
     ]
    }
   ],
   "source": [
    "assert len(FILTERS) == len(KERNEL_SIZE)\n",
    "submodels = []\n",
    "for filters, kernel in zip(FILTERS, KERNEL_SIZE):\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(vocab_size, \n",
    "                           DIM, \n",
    "                           name='emb-{}'.format(filters)))\n",
    "    submodel.add(Dropout(DROPOUT))\n",
    "    submodel.add(Conv1D(filters=filters,\n",
    "                        kernel_size=kernel,\n",
    "                        padding='same',\n",
    "                        activation='relu'))\n",
    "    submodel.add(MaxPooling1D(pool_size=2))\n",
    "    submodels.append(submodel)\n",
    "model = Sequential()\n",
    "model.add(Merge(submodels, mode=\"concat\"))\n",
    "model.add(LSTM(128, return_sequences=True, dropout=DROPOUT))\n",
    "model.add(LSTM(64, dropout=DROPOUT))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "print('Compiling model')\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizers.rmsprop(lr=0.002),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10381 samples, validate on 2596 samples\n",
      "Epoch 1/10\n",
      "10381/10381 [==============================] - 57s - loss: 52.3089 - mean_absolute_error: 3.7541 - val_loss: 53.3716 - val_mean_absolute_error: 4.8859\n",
      "Epoch 2/10\n",
      "10381/10381 [==============================] - 55s - loss: 25.0509 - mean_absolute_error: 2.3797 - val_loss: 19.1682 - val_mean_absolute_error: 2.1142\n",
      "Epoch 3/10\n",
      "10381/10381 [==============================] - 55s - loss: 13.2665 - mean_absolute_error: 1.7417 - val_loss: 13.6613 - val_mean_absolute_error: 1.7711\n",
      "Epoch 4/10\n",
      "10381/10381 [==============================] - 54s - loss: 7.2875 - mean_absolute_error: 1.3650 - val_loss: 5.9018 - val_mean_absolute_error: 1.2794\n",
      "Epoch 5/10\n",
      "10381/10381 [==============================] - 54s - loss: 4.7221 - mean_absolute_error: 1.1626 - val_loss: 4.5198 - val_mean_absolute_error: 1.1273\n",
      "Epoch 6/10\n",
      "10381/10381 [==============================] - 55s - loss: 4.3719 - mean_absolute_error: 1.1112 - val_loss: 4.2529 - val_mean_absolute_error: 1.1154\n",
      "Epoch 7/10\n",
      "10381/10381 [==============================] - 55s - loss: 3.4232 - mean_absolute_error: 1.0150 - val_loss: 3.9404 - val_mean_absolute_error: 1.1091\n",
      "Epoch 8/10\n",
      "10381/10381 [==============================] - 54s - loss: 2.9933 - mean_absolute_error: 0.9561 - val_loss: 4.2398 - val_mean_absolute_error: 1.1103\n",
      "Epoch 9/10\n",
      "10381/10381 [==============================] - 54s - loss: 2.9486 - mean_absolute_error: 0.9174 - val_loss: 5.1145 - val_mean_absolute_error: 1.2136\n",
      "Epoch 10/10\n",
      "10381/10381 [==============================] - 54s - loss: 2.5650 - mean_absolute_error: 0.8703 - val_loss: 4.8465 - val_mean_absolute_error: 1.3172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87f54b6a20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train] * BRANCHES,\n",
    "          y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=([X_test] * BRANCHES, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score: 0.9417960480384787\n",
      "explained variance score : 0.9439420700073242\n",
      "kappa: 0.9687792624798369\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_test, y_test, BRANCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Усредняем веса 4 матриц весов эмбеддингов\n"
     ]
    }
   ],
   "source": [
    "embedding = average_weights_embeddings(FILTERS, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово! Загружено 41432 векторов слов размерностью 100.\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(embedding, word_index, 'embeddings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
